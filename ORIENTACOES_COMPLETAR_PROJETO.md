# Orienta√ß√µes para Completar o Projeto

## ‚úÖ O que j√° foi feito:

1. **Introdu√ß√£o e Objetivos** - Adicionada se√ß√£o markdown no in√≠cio do notebook
2. **Imports atualizados** - Inclu√≠das todas as bibliotecas necess√°rias (statsmodels, scipy, pycaret, etc.)
3. **Requirements.txt** - Atualizado com scipy e pycaret

## üîß O que precisa ser corrigido:

### 1. C√©lula 3 - Limpeza de Dados
**Problema**: Tenta converter coluna 'Date' que n√£o existe no dataset de pre√ßos de casas.

**Solu√ß√£o**: Substituir por:
```python
## 3Ô∏è‚É£ LIMPEZA E TRATAMENTO DE DADOS

# Passo 1: Remover coluna Id
df = df.drop(columns=['Id'])

# Passo 2: Remover duplicatas
duplicates_before = len(df)
df = df.drop_duplicates()
print(f"Linhas removidas por duplica√ß√£o: {duplicates_before - len(df)}")

# Passo 3: Remover colunas com >50% de nulos
cols_to_drop = missing_analysis[missing_analysis['Percentual (%)'] > 50]['Coluna'].tolist()
cols_to_drop = [col for col in cols_to_drop if col != 'Id']
print(f"\nColunas removidas (>50% nulos): {cols_to_drop}")
df = df.drop(columns=cols_to_drop)

# Passo 4: Tratamento de nulos
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()

if 'SalePrice' in numeric_cols:
    numeric_cols.remove('SalePrice')

for col in numeric_cols:
    if df[col].isnull().sum() > 0:
        df[col] = df[col].fillna(df[col].median())

for col in categorical_cols:
    if df[col].isnull().sum() > 0:
        mode_value = df[col].mode()
        if len(mode_value) > 0:
            df[col] = df[col].fillna(mode_value[0])
        else:
            df[col] = df[col].fillna('None')

# Passo 5: Detectar outliers em SalePrice
Q1 = df['SalePrice'].quantile(0.25)
Q3 = df['SalePrice'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
outliers = df[(df['SalePrice'] < lower_bound) | (df['SalePrice'] > upper_bound)]
print(f"\nOutliers detectados em SalePrice: {len(outliers)}")
print(f"Limites: [{lower_bound:.2f}, {upper_bound:.2f}]")

print(f"\nDimens√µes ap√≥s limpeza: {df.shape}")
print(f"Valores faltantes restantes: {df.isnull().sum().sum()}")
```

### 2. C√©lula 4 - EDA
**Problema**: Referencia 'RainTomorrow' que n√£o existe.

**Solu√ß√£o**: Substituir por an√°lise de SalePrice:
```python
## 4Ô∏è‚É£ AN√ÅLISE EXPLORAT√ìRIA (EDA)

# Estat√≠sticas descritivas
print("Estat√≠sticas Descritivas:\n")
print(df.describe().T)

# Distribui√ß√£o da vari√°vel-alvo (SalePrice)
print("\n\nDistribui√ß√£o de SalePrice:")
print(df['SalePrice'].describe())

# Visualiza√ß√£o da vari√°vel-alvo
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Histograma
axes[0].hist(df['SalePrice'], bins=50, color='skyblue', edgecolor='black')
axes[0].set_title('Distribui√ß√£o de SalePrice', fontsize=12, fontweight='bold')
axes[0].set_xlabel('Pre√ßo de Venda ($)')
axes[0].set_ylabel('Frequ√™ncia')

# Boxplot
axes[1].boxplot(df['SalePrice'], vert=True)
axes[1].set_title('Boxplot de SalePrice', fontsize=12, fontweight='bold')
axes[1].set_ylabel('Pre√ßo de Venda ($)')

plt.tight_layout()
plt.show()
```

### 3. C√©lula 5 - Correla√ß√µes
**Problema**: Referencia 'RainTomorrow_encoded'.

**Solu√ß√£o**: Substituir por an√°lise de correla√ß√£o com SalePrice:
```python
## 5Ô∏è‚É£ AN√ÅLISE DE CORRELA√á√ïES E RELA√á√ïES

# Matriz de correla√ß√£o
plt.figure(figsize=(14, 10))
numeric_data = df.select_dtypes(include=[np.number])
correlation_matrix = numeric_data.corr()

sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0,
            square=True, linewidths=0.5, cbar_kws={"shrink": 0.8})
plt.title('Matriz de Correla√ß√£o', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# Correla√ß√µes com SalePrice
print("\nCorrela√ß√£o com SalePrice (Top 15):")
correlations_with_target = correlation_matrix['SalePrice'].sort_values(ascending=False)
print(correlations_with_target[1:16])  # Exclui a auto-correla√ß√£o
```

### 4. C√©lula 6 - Prepara√ß√£o para Modelagem
**Problema**: Referencia 'RainTomorrow', 'Date', 'Location'.

**Solu√ß√£o**: Substituir por:
```python
## 6Ô∏è‚É£ PREPARA√á√ÉO PARA MODELAGEM

# Preparar features e target para REGRESS√ÉO
X_reg = df.drop(['SalePrice'], axis=1)
y_reg = df['SalePrice']

# Codificar vari√°veis categ√≥ricas
le_dict = {}
for col in X_reg.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    X_reg[col] = le.fit_transform(X_reg[col].astype(str))
    le_dict[col] = le

# Normalizar features num√©ricas
scaler = StandardScaler()
X_reg_scaled = scaler.fit_transform(X_reg)
X_reg_scaled = pd.DataFrame(X_reg_scaled, columns=X_reg.columns)

# Split para REGRESS√ÉO: Train (60%), Validation (20%), Test (20%)
X_train_reg, X_temp_reg, y_train_reg, y_temp_reg = train_test_split(
    X_reg_scaled, y_reg, test_size=0.4, random_state=42)
X_val_reg, X_test_reg, y_val_reg, y_test_reg = train_test_split(
    X_temp_reg, y_temp_reg, test_size=0.5, random_state=42)

print(f"Shapes ap√≥s split (REGRESS√ÉO):")
print(f"  Train: X={X_train_reg.shape}, y={y_train_reg.shape}")
print(f"  Validation: X={X_val_reg.shape}, y={y_val_reg.shape}")
print(f"  Test: X={X_test_reg.shape}, y={y_test_reg.shape}")

# Criar vari√°vel de CLASSIFICA√á√ÉO (acima/m√©dia = 1, abaixo = 0)
median_price = y_reg.median()
y_class = (y_reg > median_price).astype(int)

print(f"\nVari√°vel de Classifica√ß√£o criada:")
print(f"  M√©dia de SalePrice: {y_reg.mean():.2f}")
print(f"  Mediana de SalePrice: {median_price:.2f}")
print(f"  Distribui√ß√£o: {y_class.value_counts().to_dict()}")
print(f"  Propor√ß√£o: {y_class.value_counts(normalize=True).to_dict()}")

# Split para CLASSIFICA√á√ÉO
X_train_clf, X_temp_clf, y_train_clf, y_temp_clf = train_test_split(
    X_reg_scaled, y_class, test_size=0.4, random_state=42, stratify=y_class)
X_val_clf, X_test_clf, y_val_clf, y_test_clf = train_test_split(
    X_temp_clf, y_temp_clf, test_size=0.5, random_state=42, stratify=y_temp_clf)

print(f"\nShapes ap√≥s split (CLASSIFICA√á√ÉO):")
print(f"  Train: X={X_train_clf.shape}, y={y_train_clf.shape}")
print(f"  Validation: X={X_val_clf.shape}, y={y_val_clf.shape}")
print(f"  Test: X={X_test_clf.shape}, y={y_test_clf.shape}")
```

## üìã Pr√≥ximas etapas necess√°rias:

### 7. Modelos de Regress√£o
- Regress√£o Linear Simples (statsmodels + sklearn)
- Regress√£o Linear M√∫ltipla (statsmodels + sklearn)
- Regress√£o Polinomial (sklearn)
- M√©tricas: MAE, RMSE, R¬≤
- Diagn√≥sticos: normalidade, homocedasticidade, VIF

### 8. Modelos de Classifica√ß√£o
- Naive Bayes
- Regress√£o Log√≠stica
- M√©tricas: accuracy, precision, recall, F1, AUC-ROC, matriz de confus√£o

### 9. Otimiza√ß√£o
- Valida√ß√£o cruzada
- PyCaret: compare_models, tune_model
- Sklearn: GridSearchCV, RandomizedSearchCV

### 10. Testes Estat√≠sticos
- t-test
- ANOVA
- Qui-quadrado

### 11. Conclus√µes
- Resumo de resultados
- Limita√ß√µes e vieses
- Trade-offs
- Refer√™ncias

## üéØ Estrutura Final Esperada:

1. Introdu√ß√£o e Objetivos ‚úÖ
2. Imports ‚úÖ
3. Carregamento de Dados ‚úÖ
4. An√°lise de Valores Ausentes ‚úÖ
5. Limpeza e Tratamento ‚ö†Ô∏è (precisa corre√ß√£o)
6. EDA ‚ö†Ô∏è (precisa corre√ß√£o)
7. Correla√ß√µes ‚ö†Ô∏è (precisa corre√ß√£o)
8. Prepara√ß√£o para Modelagem ‚ö†Ô∏è (precisa corre√ß√£o)
9. Modelos de Regress√£o ‚ùå (a implementar)
10. Modelos de Classifica√ß√£o ‚ùå (a implementar)
11. Avalia√ß√£o e Diagn√≥sticos ‚ùå (a implementar)
12. Otimiza√ß√£o ‚ùå (a implementar)
13. Conclus√µes ‚ùå (a implementar)

